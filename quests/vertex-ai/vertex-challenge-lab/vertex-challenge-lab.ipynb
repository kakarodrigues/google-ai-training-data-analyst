{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19fe218-8272-4a78-95dc-b45c7944d26d",
   "metadata": {},
   "source": [
    "# Construindo e implantando soluções de aprendizado de máquina com Vertex AI: Challenge Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e5394-d8e8-4b56-99a1-f7c3b0f574f4",
   "metadata": {},
   "source": [
    "Este laboratório de desafios é recomendado para alunos que se inscreveram no [**Construindo e implantando soluções de aprendizado de máquina com o Vertex AI**](). Você receberá um cenário e um conjunto de tarefas. Em vez de seguir instruções passo a passo, você usará as habilidades aprendidas nos laboratórios da missão para descobrir como concluir as tarefas por conta própria! Um sistema de pontuação automatizado (mostrado na página de instruções do laboratório do Qwiklabs) fornecerá feedback sobre se você concluiu suas tarefas corretamente.\n",
    "\n",
    "Ao participar de um laboratório de desafio, você não aprenderá os conceitos do Google Cloud. Para criar a solução para o desafio apresentado, use as habilidades aprendidas nos laboratórios da Quest da qual este laboratório de desafio faz parte. Espera-se que você amplie suas habilidades aprendidas e complete todos os comentários **`TODO:`** neste caderno.\n",
    "\n",
    "Você está pronto para o desafio?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908fb9b-2048-48fc-a42c-2fdf76aea51e",
   "metadata": {},
   "source": [
    "## Cenário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefbdce5-4287-4740-bdbd-729d15d8ab7f",
   "metadata": {},
   "source": [
    "Você foi contratado recentemente como engenheiro de aprendizado de máquina em um site de resenhas de filmes de startups. Seu gerente encarregou você de criar um modelo de aprendizado de máquina para classificar o sentimento das resenhas de filmes do usuário como positivo ou negativo. Essas previsões serão usadas como uma entrada nos sistemas de classificação de filmes downstream e para exibir as principais críticas de apoio e críticas no aplicativo do site do filme. O desafio: seus requisitos de negócios são que você tenha apenas 6 semanas para produzir um modelo que atinja mais de 75% de precisão para melhorar uma solução bootstrap existente. Além disso, depois de fazer algumas análises exploratórias no data warehouse de sua startup, você descobriu que tem apenas um pequeno conjunto de dados de 50 mil revisões de texto para construir uma solução de melhor desempenho.\n",
    "\n",
    "Para criar e implantar rapidamente um modelo de aprendizado de máquina de alto desempenho com dados limitados, você passará pelo treinamento e pela implantação de um classificador de sentimento personalizado do TensorFlow BERT para previsões on-line no [Vertex AI](https://cloud.google.com/vertex-ai) do Google Cloud Plataform. A Vertex AI é a plataforma de desenvolvimento de aprendizado de máquina de última geração do Google Cloud, na qual você pode aproveitar os componentes pré-criados de ML mais recentes e o AutoML para melhorar significativamente sua produtividade de desenvolvimento, dimensionar seu fluxo de trabalho e tomar decisões com seus dados e acelerar o tempo de retorno.\n",
    "\n",
    "![Vertex AI: Challenge Lab](./images/vertex-challenge-lab.png \"Vertex Challenge Lab\")\n",
    "\n",
    "Primeiro, você irá progredir através de um fluxo de trabalho de experimentação típico onde você construirá seu modelo a partir de componentes BERT pré-treinados das camadas de classificação TF-Hub e `tf.keras` para treinar e avaliar seu modelo em um Vertex Notebook. Em seguida, você empacotará seu código de modelo em um contêiner do Docker para treinar no Vertex AI do Google Cloud. Por fim, você definirá e executará um Kubeflow Pipeline no Vertex Pipelines que treina e implanta seu modelo em um Vertex Endpoint que você consultará para previsões online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955d75d-cfa4-43af-8783-d2aec5ae525e",
   "metadata": {},
   "source": [
    "## Objetivos de aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386b37c-2ce1-4b1f-8c90-b83bda6075c8",
   "metadata": {},
   "source": [
    "* Treine um modelo do TensorFlow localmente em um [**Vertex Notebook**](https://cloud.google.com/vertex-ai/docs/general/notebooks?hl=sv) hospedado.\n",
    "* Coloque seu código de treinamento em contêiner com o [**Cloud Build**](https://cloud.google.com/build) e envie-o para o [**Google Cloud Artifact Registry**](https://cloud.google.com/artifact-registry).\n",
    "* Defina um pipeline usando o [**Kubeflow Pipelines (KFP) V2 SDK**](https://www.kubeflow.org/docs/components/pipelines/sdk/v2/v2-compatibility) para treinar e implantar seu modelo em [**Vertex Pipelines**](https://cloud.google.com/vertex-ai/docs/pipelines).\n",
    "* Consulte seu modelo em um [**Vertex Endpoint**](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions) usando previsões on-line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23538a-e809-4747-9bd4-5610f8544ea1",
   "metadata": {},
   "source": [
    "## Configurações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4480c8-710c-40dd-93c2-c51e67e59760",
   "metadata": {},
   "source": [
    "### Definição de Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0feaf4-9849-4636-b736-d3cd8a051579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicione dependências de biblioteca instaladas à variável Python PATH.\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68df5dd-c456-4edd-8f58-71597f10c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupere e defina as variáveis de ambiente PROJECT_ID e REGION.\n",
    "# TODO: preencha PROJECT_ID.\n",
    "PROJECT_ID = \"\"\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3912f9-6c12-439f-8613-cc60c286b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Crie um bucket globalmente exclusivo do Google Cloud Storage para armazenamento de artefatos.\n",
    "GCS_BUCKET = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931ae91-3ba1-437a-9c37-187a41a3d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l $REGION $GCS_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebbc2b-21ad-47f0-829f-9beba0deba9d",
   "metadata": {},
   "source": [
    "### Importe as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf558fc-d0fc-4452-8281-7d7cd0cffe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "# Bibliotecas de construção de modelos do TensorFlow.\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Recrie o otimizador AdamW usado no documento BERT original.\n",
    "from official.nlp import optimization  \n",
    "\n",
    "# Bibliotecas para métricas de treinamento de modelos de dados e gráficos.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importe o SDK do Vertex AI Python.\n",
    "from google.cloud import aiplatform as vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296167a-13b9-4895-be8b-b3b49fad5d47",
   "metadata": {},
   "source": [
    "### Inicialize o SDK do Vertex AI Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c178b0-0edb-4e4b-abb4-d3cc0bd676de",
   "metadata": {},
   "source": [
    "Inicialize o Vertex AI Python SDK com seu projeto, região e bucket do Google Cloud Storage do GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43371e-2c64-4a76-8698-fa768043dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2917411-811c-46dd-8eda-e8ef579c568d",
   "metadata": {},
   "source": [
    "## Crie e treine seu modelo localmente em um Vertex Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc8cc5-ed5e-457a-b5f5-475bacca4611",
   "metadata": {},
   "source": [
    "Observação: este laboratório adapta e estende o [tutorial de classificação de texto TensorFlow BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) oficial para utilizar os serviços Vertex AI. Consulte o tutorial para obter cobertura adicional sobre o ajuste fino de modelos BERT usando o TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338a818-18e5-4b0b-b37d-b387577a08ef",
   "metadata": {},
   "source": [
    "### Conjunto de dados do laboratório"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfb91d-6060-4d00-a1c3-299ee6027b76",
   "metadata": {},
   "source": [
    "Neste laboratório, você usará o [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment) que contém o texto de 50.000 resenhas de filmes do Internet Movie Database. Eles são divididos em 25.000 avaliações para treinamento e 25.000 avaliações para teste. Os conjuntos de treinamento e teste são balanceados, o que significa que contêm um número igual de avaliações positivas e negativas. O código de ingestão e processamento de dados foi fornecido para você abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef491df4-c35f-4555-a6b6-96114c3d3c6e",
   "metadata": {},
   "source": [
    "### Importe o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee70d2c-c0e3-4c75-9bc6-b42dad6c7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "LOCAL_DATA_DIR = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889f275-ce52-4108-9f7f-7cf824184f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(data_url, local_data_dir):\n",
    "    \"\"\"Download dataset.\n",
    "    Args:\n",
    "      data_url(str): Source data URL path.\n",
    "      local_data_dir(str): Local data download directory path.\n",
    "    Returns:\n",
    "      dataset_dir(str): Local unpacked data directory path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_data_dir):\n",
    "        os.makedirs(local_data_dir)\n",
    "    \n",
    "    dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb_v1.tar.gz\",\n",
    "      origin=data_url,\n",
    "      untar=True,\n",
    "      cache_dir=local_data_dir,\n",
    "      cache_subdir=\"\")\n",
    "    \n",
    "    dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
    "    \n",
    "    train_dir = os.path.join(dataset_dir, \"train\")\n",
    "    \n",
    "    # Remova as pastas não utilizadas para facilitar o carregamento dos dados.\n",
    "    remove_dir = os.path.join(train_dir, \"unsup\")\n",
    "    shutil.rmtree(remove_dir)\n",
    "    \n",
    "    return dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f906a4-64a0-45ae-b376-757ef0f661fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = download_data(data_url=DATA_URL, local_data_dir=LOCAL_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a61fa-cf55-470f-9837-c783c4bcccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie um dicionário para adicionar de forma iterativa o pipeline de dados e os hiperparâmetros de treinamento do modelo.\n",
    "HPARAMS = {\n",
    "    # Defina uma semente de amostragem aleatória para evitar vazamento de dados em divisões de dados de arquivos.\n",
    "    \"seed\": 42,\n",
    "    # Número de exemplos de treinamento e inferência.\n",
    "    \"batch-size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeea425-d288-44a7-9958-f2b1f48f9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(dataset_dir, hparams):\n",
    "    \"\"\"Load pre-split tf.datasets.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      raw_train_ds(tf.dataset): Train split dataset (20k examples).\n",
    "      raw_val_ds(tf.dataset): Validation split dataset (5k examples).\n",
    "      raw_test_ds(tf.dataset): Test split dataset (25k examples).\n",
    "    \"\"\"    \n",
    "\n",
    "    raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'train'),\n",
    "        batch_size=hparams['batch-size'],\n",
    "        validation_split=0.2,\n",
    "        subset='training',\n",
    "        seed=hparams['seed'])    \n",
    "\n",
    "    raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'train'),\n",
    "        batch_size=hparams['batch-size'],\n",
    "        validation_split=0.2,\n",
    "        subset='validation',\n",
    "        seed=hparams['seed'])\n",
    "\n",
    "    raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'test'),\n",
    "        batch_size=hparams['batch-size'])\n",
    "    \n",
    "    return raw_train_ds, raw_val_ds, raw_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff05aa4-d299-4c80-a29a-43c6bc3ac152",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds, raw_val_ds, raw_test_ds = load_datasets(DATASET_DIR, HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee40c0-9e37-483c-98f5-dcdb467a2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "CLASS_NAMES = raw_train_ds.class_names\n",
    "\n",
    "train_ds = raw_train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = raw_val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = raw_test_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5734e-d97c-484d-9f52-f6fb4e153ef0",
   "metadata": {},
   "source": [
    "Let's print a few example reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d794068-817c-4cb8-8e4c-c49860d0c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Review {i}: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({CLASS_NAMES[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e6686-2fa1-453c-8259-8e5a87cba023",
   "metadata": {},
   "source": [
    "### Escolha um modelo BERT pré-treinado para ajustar e obter maior precisão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3502ad71-5747-4a11-9122-0b5c7b2049cd",
   "metadata": {},
   "source": [
    "[**Bidirectional Encoder Representations from Transformers (BERT)**](https://arxiv.org/abs/1810.04805v2) é um modelo de representação de texto baseado em transformador pré-treinado em grandes conjuntos de dados (mais de 3 bilhões de palavras) que pode ser ajustado para resultados de última geração em muitas tarefas de processamento de linguagem natural (NLP). Desde o lançamento em 2018 pelos pesquisadores do Google, ele transformou o campo da pesquisa em PNL e passou a fazer parte de melhorias significativas na [Pesquisa do Google](https://www.blog.google/products/search/search-language- compreensão-bert).\n",
    "\n",
    "Para atender aos seus requisitos de negócios de alcançar maior precisão em um pequeno conjunto de dados (exemplos de treinamento de 20 mil), você usará uma técnica chamada aprendizado de transferência para combinar um codificador BERT pré-treinado e camadas de classificação para ajustar um novo modelo de melhor desempenho para classificação de sentimento binário ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd347cc-5f87-4b89-8833-6df850729ec3",
   "metadata": {},
   "source": [
    "Para este laboratório, você usará um modelo BERT menor que troca alguma precisão por tempos de treinamento mais rápidos.\n",
    "\n",
    "Os modelos Small BERT são instâncias da arquitetura BERT original com um número menor L de camadas (ou seja, blocos residuais) combinados com um tamanho oculto H menor e um número menor A correspondente de cabeças de atenção, conforme publicado por\n",
    "\n",
    "Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: [\"Alunos bem lidos aprendem melhor: sobre a importância dos modelos compactos de pré-treinamento\"](https://arxiv.org/abs/1908.08962), 2019.\n",
    "\n",
    "Eles têm a mesma arquitetura geral, mas menos e/ou blocos Transformer menores, o que permite explorar compensações entre velocidade, tamanho e qualidade.\n",
    "\n",
    "Os seguintes modelos de pré-processamento e codificador no formato TensorFlow 2 SavedModel usam a implementação do BERT do [repositório Github de modelos do TensorFlow](https://github.com/tensorflow/models/tree/master/official/nlp/bert) com o pesos treinados divulgados pelos autores do Small BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22196658-1c30-49c7-8485-5d933fc8988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPARAMS.update({\n",
    "    # TF Hub BERT modules.\n",
    "    \"tfhub-bert-preprocessor\": \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "    \"tfhub-bert-encoder\": \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50b93a-df95-47d9-bc14-14502ae4eb54",
   "metadata": {},
   "source": [
    "As entradas de texto precisam ser transformadas em ids de token numéricos e organizadas em vários tensores antes de serem inseridas no BERT. O TensorFlow Hub fornece um modelo de pré-processamento correspondente para cada um dos modelos BERT discutidos acima, que implementa essa transformação usando operações TF da biblioteca TF.text. Como esse pré-processador de texto é um modelo do TensorFlow, ele pode ser incluído diretamente no seu modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e50df-9d35-4116-a167-8353046bf6b9",
   "metadata": {},
   "source": [
    "Para ajuste fino, você usará o mesmo otimizador com o qual o BERT foi originalmente treinado: os \"Momentos Adaptativos\" (Adam). Esse otimizador minimiza a perda de previsão e faz a regularização por decaimento de peso (sem usar momentos), que também é conhecido como [AdamW](https://arxiv.org/abs/1711.05101)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26396cb1-fc24-4e96-bef2-6fc8e2d500a6",
   "metadata": {},
   "source": [
    "Para a taxa de aprendizado `inicial-learning-rate`, você usará o mesmo cronograma do pré-treinamento BERT: decaimento linear de uma taxa de aprendizado inicial nocional, prefixada com uma fase de aquecimento linear nos primeiros 10% das etapas de treinamento ` n_warmup_steps`. De acordo com o documento BERT, a taxa de aprendizado inicial é menor para ajuste fino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b634139-a0d1-41e7-be23-c6e580a4f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPARAMS.update({\n",
    "    # Hiperparâmetros de treinamento do modelo para ajuste fino e regularização.\n",
    "    \"epochs\": 3,\n",
    "    \"initial-learning-rate\": 3e-5,\n",
    "    \"dropout\": 0.1 \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e415aeb-5ab2-42ac-904a-ae0649d45a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = HPARAMS['epochs']\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "n_train_steps = steps_per_epoch * epochs\n",
    "n_warmup_steps = int(0.1 * n_train_steps)    \n",
    "\n",
    "OPTIMIZER = optimization.create_optimizer(init_lr=HPARAMS['initial-learning-rate'],\n",
    "                                          num_train_steps=n_train_steps,\n",
    "                                          num_warmup_steps=n_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b4646-ce95-47c4-b1f7-886f59980386",
   "metadata": {},
   "source": [
    "### Crie e compile um classificador de sentimento do TensorFlow BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80920377-4596-4dbd-8eb7-8580327fdb24",
   "metadata": {},
   "source": [
    "Em seguida, você definirá e compilará seu modelo montando componentes pré-construídos do TF-Hub e camadas tf.keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289da96f-2aad-4c34-85ce-5916ea98778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_classifier(hparams, optimizer):\n",
    "    \"\"\"Define and compile a TensorFlow BERT sentiment classifier.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      model(tf.keras.Model): A compiled TensorFlow model.\n",
    "    \"\"\"\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    # TODO: Adicione um hub.KerasLayer para pré-processamento de texto BERT usando o dict hparams.\n",
    "    # Nomeie a camada como 'preprocessing' e armazene na variável preprocessor.\n",
    "    \n",
    "    encoder_inputs = preprocessor(text_input)\n",
    "    # TODO: Adicione um hub.KerasLayer treinável para codificação de texto BERT usando o dict hparams.\n",
    "    # Nomeie a camada 'BERT_encoder' e armazene na variável encoder.\n",
    "\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    # Para o ajuste fino, você usará o array `pooled_output` que representa\n",
    "    # cada sequência de entrada como um todo. A forma é [batch_size, H].\n",
    "    # Você pode pensar nisso como uma incorporação para toda a crítica do filme.\n",
    "    classifier = outputs['pooled_output']\n",
    "    # Adicione dropout para evitar overfitting durante o ajuste fino do modelo.\n",
    "    classifier = tf.keras.layers.Dropout(hparams['dropout'], name='dropout')(classifier)\n",
    "    classifier = tf.keras.layers.Dense(1, activation=None, name='classifier')(classifier)\n",
    "    model = tf.keras.Model(text_input, classifier, name='bert-sentiment-classifier')\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b72cc-9e1c-49c9-8a90-7b09b6108f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_text_classifier(HPARAMS, OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8198df2-c15a-4f79-a154-83c941aba5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize seu classificador de sentimento BERT ajustado.\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfeb71d-4e19-4759-8f5c-293c8c7cee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_REVIEW = ['this is such an amazing movie!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2112f-9f04-470e-8636-38ea948cba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_RAW_RESULT = model(tf.constant(TEST_REVIEW))\n",
    "print(BERT_RAW_RESULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53dd9fa-d0a4-46ab-a123-3065b8fde7c8",
   "metadata": {},
   "source": [
    "### Treine e avalie a classificação de sentimento BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f008fc-f696-4b71-9011-2a897d268795",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPARAMS.update({\n",
    "    # TODO: Salve seu classificador de sentimento BERT localmente.\n",
    "    # Dica: Salve-o em './bert-sentiment-classifier-local'. Observe o nome da chave em model.save().\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd08f3-988f-408a-b694-af4702de85ba",
   "metadata": {},
   "source": [
    "**Note:** treinar seu modelo localmente levará cerca de 8 a 10 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24660956-d60a-4c25-a654-7b192a01a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(hparams):\n",
    "    \"\"\"Train and evaluate TensorFlow BERT sentiment classifier.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      history(tf.keras.callbacks.History): Keras callback that records training event history.\n",
    "    \"\"\"\n",
    "    # dataset_dir = download_data(data_url, local_data_dir)\n",
    "    raw_train_ds, raw_val_ds, raw_test_ds = load_datasets(DATASET_DIR, hparams)\n",
    "    \n",
    "    train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)     \n",
    "    \n",
    "    epochs = hparams['epochs']\n",
    "    steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "    n_train_steps = steps_per_epoch * epochs\n",
    "    n_warmup_steps = int(0.1 * n_train_steps)    \n",
    "    \n",
    "    optimizer = optimization.create_optimizer(init_lr=hparams['initial-learning-rate'],\n",
    "                                              num_train_steps=n_train_steps,\n",
    "                                              num_warmup_steps=n_warmup_steps,\n",
    "                                              optimizer_type='adamw')    \n",
    "    \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = build_text_classifier(hparams=hparams, optimizer=optimizer)\n",
    "    \n",
    "    logging.info(model.summary())\n",
    "        \n",
    "    history = model.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=epochs)  \n",
    "    \n",
    "    logging.info(\"Test accuracy: %s\", model.evaluate(test_ds))\n",
    "\n",
    "    # Export Keras model in TensorFlow SavedModel format.\n",
    "    model.save(hparams['model-dir'])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549e700-bf6a-415a-bb35-01150d9535e5",
   "metadata": {},
   "source": [
    "Baseado no objeto `History` retornado por `model.fit()`. Você pode plotar a perda de treinamento e validação para comparação, bem como a precisão de treinamento e validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cab23-fbf0-44d2-9ee0-e2dd83390fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_evaluate(HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91980420-8451-4869-b189-2b3693131ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcbcff-18d5-4448-b695-fd6bf3189c2f",
   "metadata": {},
   "source": [
    "Nesse gráfico, as linhas vermelhas representam a perda e a precisão do treinamento e as linhas azuis são a perda e a precisão da validação. Com base nos gráficos acima, você deve ver a precisão do modelo em torno de 78-80%, o que excede sua meta de requisitos de negócios de mais de 75% de precisão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc3fed-aa4c-40b2-8c44-19be21ba4689",
   "metadata": {},
   "source": [
    "## Conteinerize seu código de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3905338-288b-4565-9abb-9053d7559315",
   "metadata": {},
   "source": [
    "Agora que você treinou e avaliou seu modelo localmente em um Vertex Notebook como parte de um fluxo de trabalho de experimentação, sua próxima etapa é treinar e implantar seu modelo na plataforma Vertex AI do Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb61ec-cb3c-43bd-9d75-9d61ff52848e",
   "metadata": {},
   "source": [
    "Para treinar seu classificador BERT no Google Cloud, você empacotará seus scripts de treinamento Python e escreverá um Dockerfile que contém instruções sobre o código do modelo de ML, dependências e instruções de execução. Você criará seu contêiner personalizado com o Cloud Build, cujas instruções são especificadas em `cloudbuild.yaml` e publicará seu contêiner em seu Artifact Registry. Esse fluxo de trabalho oferece a oportunidade de usar o mesmo contêiner para executar como parte de um fluxo de trabalho [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) portátil e escalável.\n",
    "\n",
    "\n",
    "Você passará pela criação da seguinte estrutura de projeto para o código do modo ML:\n",
    "```\n",
    "|--/bert-sentiment-classifier\n",
    "   |--/trainer\n",
    "      |--__init__.py\n",
    "      |--model.py\n",
    "      |--task.py\n",
    "   |--Dockerfile\n",
    "   |--cloudbuild.yaml\n",
    "   |--requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033e3c3-9dad-49d8-b53c-fd48113a8f90",
   "metadata": {},
   "source": [
    "### 1. Escreva um script de treinamento `model.py`\n",
    "\n",
    "Primeiro, você organizará seu código de treinamento do modelo TensorFlow local acima em um script de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129184d-15ed-4ebe-bbdc-6c2687eb18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"bert-sentiment-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594afe7-b9e0-4957-9156-d2e595fde62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {MODEL_DIR}/trainer/model.py\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from official.nlp import optimization\n",
    "\n",
    "DATA_URL = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "LOCAL_DATA_DIR = './tmp/data'\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def download_data(data_url, local_data_dir):\n",
    "    \"\"\"Download dataset.\n",
    "    Args:\n",
    "      data_url(str): Source data URL path.\n",
    "      local_data_dir(str): Local data download directory path.\n",
    "    Returns:\n",
    "      dataset_dir(str): Local unpacked data directory path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_data_dir):\n",
    "        os.makedirs(local_data_dir)\n",
    "    \n",
    "    dataset = tf.keras.utils.get_file(\n",
    "      fname='aclImdb_v1.tar.gz',\n",
    "      origin=data_url,\n",
    "      untar=True,\n",
    "      cache_dir=local_data_dir,\n",
    "      cache_subdir=\"\")\n",
    "    \n",
    "    dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "    \n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    \n",
    "    # Remove unused folders to make it easier to load the data.\n",
    "    remove_dir = os.path.join(train_dir, 'unsup')\n",
    "    shutil.rmtree(remove_dir)\n",
    "    \n",
    "    return dataset_dir\n",
    "\n",
    "\n",
    "def load_datasets(dataset_dir, hparams):\n",
    "    \"\"\"Load pre-split tf.datasets.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      raw_train_ds(tf.dataset): Train split dataset (20k examples).\n",
    "      raw_val_ds(tf.dataset): Validation split dataset (5k examples).\n",
    "      raw_test_ds(tf.dataset): Test split dataset (25k examples).\n",
    "    \"\"\"    \n",
    "\n",
    "    raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'train'),\n",
    "        batch_size=hparams['batch-size'],\n",
    "        validation_split=0.2,\n",
    "        subset='training',\n",
    "        seed=hparams['seed'])    \n",
    "\n",
    "    raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'train'),\n",
    "        batch_size=hparams['batch-size'],\n",
    "        validation_split=0.2,\n",
    "        subset='validation',\n",
    "        seed=hparams['seed'])\n",
    "\n",
    "    raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'test'),\n",
    "        batch_size=hparams['batch-size'])\n",
    "    \n",
    "    return raw_train_ds, raw_val_ds, raw_test_ds\n",
    "\n",
    "\n",
    "def build_text_classifier(hparams, optimizer):\n",
    "    \"\"\"Define and compile a TensorFlow BERT sentiment classifier.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      model(tf.keras.Model): A compiled TensorFlow model.\n",
    "    \"\"\"\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    # TODO: Adicione um hub.KerasLayer para pré-processamento de texto BERT usando o dict hparams.\n",
    "    # Nomeie a camada como 'preprocessing' e armazene na variável preprocessor.\n",
    "    preprocessor = hub.KerasLayer(hparams['tfhub-bert-preprocessor'], name='preprocessing')\n",
    "    encoder_inputs = preprocessor(text_input)\n",
    "    # TODO: Adicione um hub.KerasLayer treinável para codificação de texto BERT usando o dict hparams.\n",
    "    # Nomeie a camada 'BERT_encoder' e armazene na variável encoder.\n",
    "    encoder = hub.KerasLayer(hparams['tfhub-bert-encoder'], trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    # Para o ajuste fino, você usará o array `pooled_output` que representa\n",
    "    # cada sequência de entrada como um todo. A forma é [batch_size, H].\n",
    "    # Você pode pensar nisso como uma incorporação para toda a crítica do filme.\n",
    "    classifier = outputs['pooled_output']\n",
    "    # Adicione dropout para evitar overfitting durante o ajuste fino do modelo.\n",
    "    classifier = tf.keras.layers.Dropout(hparams['dropout'], name='dropout')(classifier)\n",
    "    classifier = tf.keras.layers.Dense(1, activation=None, name='classifier')(classifier)\n",
    "    model = tf.keras.Model(text_input, classifier, name='bert-sentiment-classifier')\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_evaluate(hparams):\n",
    "    \"\"\"Train and evaluate TensorFlow BERT sentiment classifier.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      history(tf.keras.callbacks.History): Keras callback that records training event history.\n",
    "    \"\"\"\n",
    "    dataset_dir = download_data(data_url=DATA_URL, \n",
    "                                local_data_dir=LOCAL_DATA_DIR)\n",
    "    \n",
    "    raw_train_ds, raw_val_ds, raw_test_ds = load_datasets(dataset_dir=dataset_dir,\n",
    "                                                          hparams=hparams)\n",
    "    \n",
    "    train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)     \n",
    "    \n",
    "    epochs = hparams['epochs']\n",
    "    steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "    n_train_steps = steps_per_epoch * epochs\n",
    "    n_warmup_steps = int(0.1 * n_train_steps)    \n",
    "    \n",
    "    optimizer = optimization.create_optimizer(init_lr=hparams['initial-learning-rate'],\n",
    "                                              num_train_steps=n_train_steps,\n",
    "                                              num_warmup_steps=n_warmup_steps,\n",
    "                                              optimizer_type='adamw')    \n",
    "    \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = build_text_classifier(hparams=hparams, optimizer=optimizer)\n",
    "        logging.info(model.summary())\n",
    "        \n",
    "    history = model.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=epochs)  \n",
    "    \n",
    "    logging.info(\"Test accuracy: %s\", model.evaluate(test_ds))\n",
    "\n",
    "    # Exporte o modelo Keras no formato TensorFlow SavedModel.\n",
    "    model.save(hparams['model-dir'])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e16b936-d93a-4411-a494-aacda93b05f4",
   "metadata": {},
   "source": [
    "### 2. Escreva um arquivo `task.py` como um ponto de entrada para seu contêiner de modelo personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17517e0b-a2ac-489a-bf03-357ace5d6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {MODEL_DIR}/trainer/task.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Argumentos de treinamento de contêiner personalizado de vértice. Eles são definidos pelo Vertex AI durante o treinamento, mas também podem ser substituídos.\n",
    "    parser.add_argument('--model-dir', dest='model-dir',\n",
    "                        default=os.environ['AIP_MODEL_DIR'], type=str, help='GCS URI for saving model artifacts.')\n",
    "\n",
    "    # Argumentos de treinamento do modelo.\n",
    "    parser.add_argument('--tfhub-bert-preprocessor', dest='tfhub-bert-preprocessor', \n",
    "                        default='https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3', type=str, help='TF-Hub URL.')\n",
    "    parser.add_argument('--tfhub-bert-encoder', dest='tfhub-bert-encoder', \n",
    "                        default='https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2', type=str, help='TF-Hub URL.')    \n",
    "    parser.add_argument('--initial-learning-rate', dest='initial-learning-rate', default=3e-5, type=float, help='Learning rate for optimizer.')\n",
    "    parser.add_argument('--epochs', dest='epochs', default=3, type=int, help='Training iterations.')    \n",
    "    parser.add_argument('--batch-size', dest='batch-size', default=32, type=int, help='Number of examples during each training iteration.')    \n",
    "    parser.add_argument('--dropout', dest='dropout', default=0.1, type=float, help='Float percentage of DNN nodes [0,1] to drop for regularization.')    \n",
    "    parser.add_argument('--seed', dest='seed', default=42, type=int, help='Random number generator seed to prevent overlap between train and val sets.')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "\n",
    "    model.train_evaluate(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503a04a-5f15-4503-91e9-1acc4353fd08",
   "metadata": {},
   "source": [
    "### 3. Escreva um `Dockerfile` para seu contêiner de modelo personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b0320-a3c2-4fbd-96f0-48ca3b49d485",
   "metadata": {},
   "source": [
    "Terceiro, você escreverá um `Dockerfile` que contém instruções para empacotar o código do seu modelo em `bert-sentiment-classifier`, bem como especificar as dependências do código do seu modelo necessárias para execução em um contêiner do Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ede10-6372-4320-89d3-264d4d1b1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {MODEL_DIR}/Dockerfile\n",
    "# Especifica a imagem de base e a marca.\n",
    "# https://cloud.google.com/vertex-ai/docs/training/pre-built-containers\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-6:latest\n",
    "\n",
    "# Define o diretório de trabalho do contêiner.\n",
    "WORKDIR /root\n",
    "\n",
    "# Copia o requirements.txt no contêiner para reduzir as chamadas de rede.\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Instala pacotes adicionais.\n",
    "RUN pip3 install -U -r requirements.txt\n",
    "\n",
    "# b/203105209 Remove o arquivo desnecessário da imagem de CPU TF2.5 para treinamento de python_module CustomJob.\n",
    "# Será removido nas imagens públicas de Vertex subsequentes.\n",
    "RUN rm -rf /var/sitecustomize/sitecustomize.py\n",
    "\n",
    "# Copia o código do treinamento para a imagem do docker.\n",
    "COPY . /trainer\n",
    "\n",
    "# Define o diretório de trabalho do contêiner.\n",
    "WORKDIR /trainer\n",
    "\n",
    "# Configura o ponto de entrada para invocar o treinamento.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2974866-46f7-4f16-b6c0-9ea420ea6d73",
   "metadata": {},
   "source": [
    "### 4. Escreva um arquivo `requirements.txt` para especificar dependências de código ML adicionais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d62327-31c6-4ae5-819c-f59fb16e58c3",
   "metadata": {},
   "source": [
    "Essas são dependências adicionais para o código do seu modelo não incluídas nas imagens do TensorFlow do Vertex pré-criadas, como TF-Hub, otimizador TensorFlow AdamW e texto do TensorFlow necessários para importar e trabalhar com modelos TensorFlow BERT pré-treinados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7619e1-fff9-4a47-90a8-3ba9b55e74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {MODEL_DIR}/requirements.txt\n",
    "tf-models-official==2.6.0\n",
    "tensorflow-text==2.6.0\n",
    "tensorflow-hub==0.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81292584-7a08-4c92-a9ba-e3dbc7005130",
   "metadata": {},
   "source": [
    "## Use o Cloud Build para criar e enviar seu contêiner de modelo ao Google Cloud Artifact Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47400c-aa7c-4929-a584-f5067bd682eb",
   "metadata": {},
   "source": [
    "Em seguida, você usará o [Cloud Build](https://cloud.google.com/build) para criar e fazer upload do seu contêiner de modelo personalizado do TensorFlow para o [Google Cloud Artifact Registry](https://cloud.google.com/artifact-registro).\n",
    "\n",
    "O Cloud Build traz reutilização e automação para sua experimentação de ML, permitindo que você crie, teste e implante de forma confiável o código do modelo de ML como parte de um fluxo de trabalho de CI/CD. O Artifact Registry fornece um repositório centralizado para você armazenar, gerenciar e proteger suas imagens de contêiner de ML. Isso permitirá que você compartilhe com segurança seu trabalho de ML com outras pessoas e reproduza os resultados do experimento.\n",
    "\n",
    "**Observação**: a etapa inicial de compilação e envio levará cerca de 16 minutos, mas o Cloud Build pode aproveitar o armazenamento em cache para compilações subsequentes mais rápidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c0d02-200f-4cc3-bdfd-ba96d233ecc4",
   "metadata": {},
   "source": [
    "### 1. Create Artifact Registry for custom container images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9918475-f6dc-4fa3-8249-47976e68f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_REGISTRY=\"bert-sentiment-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9566d-c6c4-48c8-b4f8-ad239e5ae349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: crie um Docker Artifact Registry usando a CLI gcloud. Observe os sinalizadores de formato e local de repositório necessários.\n",
    "# Link da documentação: https://cloud.google.com/sdk/gcloud/reference/artifacts/repositories/create\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900832e-de90-4ba1-ba7d-7973a1de9cc1",
   "metadata": {},
   "source": [
    "### 2. Crie instruções `cloudbuild.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580619d-957c-409f-ac15-ccbc0bb79a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME=\"bert-sentiment-classifier\"\n",
    "IMAGE_TAG=\"latest\"\n",
    "IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REGISTRY}/{IMAGE_NAME}:{IMAGE_TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24790970-988e-4694-b8cd-a2d9d500c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudbuild_yaml = f\"\"\"steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', '{IMAGE_URI}', '.' ]\n",
    "images: \n",
    "- '{IMAGE_URI}'\"\"\"\n",
    "\n",
    "with open(f\"{MODEL_DIR}/cloudbuild.yaml\", \"w\") as fp:\n",
    "    fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14b9c6-c120-482d-b5ab-c6a4c53bb205",
   "metadata": {},
   "source": [
    "### 3. Crie e envie sua imagem de contêiner para o Artifact Registry usando o Cloud Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e154b1-e584-496c-97b4-14795f80928b",
   "metadata": {},
   "source": [
    "**Observação:** seu contêiner de modelo personalizado levará cerca de 16 minutos inicialmente para ser criado e enviado ao seu Artifact Registry. O Artifact Registry é capaz de aproveitar o armazenamento em cache para que as compilações subsequentes demorem cerca de 4 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf0093-d66e-49c1-8a55-047020735e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use o Cloud Build para criar e enviar seu contêiner de modelo personalizado ao Artifact Registry.\n",
    "# Link da documentação: https://cloud.google.com/sdk/gcloud/reference/builds/submit\n",
    "# Dica: certifique-se de que o sinalizador de configuração esteja apontado para {MODEL_DIR}/cloudbuild.yaml definido acima e inclua seu diretório de modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee35ac-ab83-472d-ab18-f622f3e3bc31",
   "metadata": {},
   "source": [
    "## Defina um pipeline usando o SDK do KFP V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5556979-3583-44fd-86df-d30fab8d9464",
   "metadata": {},
   "source": [
    "Para atender aos seus requisitos de negócios e colocar seu modelo de melhor desempenho em produção para agregar valor mais rapidamente, você definirá um pipeline usando o [**Kubeflow Pipelines (KFP) V2 SDK**](https://www.kubeflow.org/docs/components/pipelines/sdk/v2/v2-compatibility) para orquestrar o treinamento e a implantação do seu modelo no [**Vertex Pipelines**](https://cloud.google.com/vertex-ai/docs/pipelines) abaixo de."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0e36b-3cb8-4660-bbb1-a5dcca49aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# google_cloud_pipeline_components includes pre-built KFP components for interfacing with Vertex AI services.\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f7070-d6e5-47ab-a860-d6a7e7892164",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP=datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "DISPLAY_NAME = \"bert-sentiment-{}\".format(TIMESTAMP)\n",
    "GCS_BASE_OUTPUT_DIR= f\"{GCS_BUCKET}/{MODEL_DIR}-{TIMESTAMP}\"\n",
    "\n",
    "USER = \"\"  # TODO: mude isso para o seu nome.\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(GCS_BUCKET, USER)\n",
    "\n",
    "print(f\"Model display name: {DISPLAY_NAME}\")\n",
    "print(f\"GCS dir for model training artifacts: {GCS_BASE_OUTPUT_DIR}\")\n",
    "print(f\"GCS dir for pipeline artifacts: {PIPELINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aecd42-c969-4ce0-a49a-9150c45a91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contêiner de serviço de modelo Vertex pré-construído para implantação.\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "SERVING_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e8dbc-04b3-4193-87f5-984d2b98a2d0",
   "metadata": {},
   "source": [
    "O pipeline consiste em três componentes:\n",
    "\n",
    "* `CustomContainerTrainingJobRunOp` [(documentação)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp): treina seu contêiner de modelo personalizado usando o Vertex Training. Isso é o mesmo que configurar um trabalho de treinamento de contêiner personalizado Vertex usando o Vertex Python SDK que você abordou no laboratório Vertex AI: Qwik Start.\n",
    "\n",
    "* `EndpointCreateOp` [(documentação)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.EndpointCreateOp): cria um recurso do Google Cloud Vertex Endpoint que mapeia recursos de máquina física com seu modelo para permitir que ele veicule previsões on-line. As previsões online têm requisitos de baixa latência; fornecer recursos para o modelo com antecedência reduz a latência.\n",
    "\n",
    "* `ModelDeployOp`[(documentação)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.ModelDeployOp): implanta seu modelo em um Vertex Prediction Endpoint para previsões online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2181f3d-10cd-49c8-8e2f-e5c314940321",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"bert-sentiment-classification\", pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    staging_bucket: str = GCS_BUCKET,\n",
    "    display_name: str = DISPLAY_NAME,    \n",
    "    container_uri: str = IMAGE_URI,\n",
    "    model_serving_container_image_uri: str = SERVING_IMAGE_URI,    \n",
    "    base_output_dir: str = GCS_BASE_OUTPUT_DIR,\n",
    "):\n",
    "    \n",
    "    #TODO: adicione e configure o componente KFP CustomContainerTrainingJobRunOp pré-criado usando\n",
    "    # os argumentos restantes no construtor do pipeline.\n",
    "    # Dica: Consulte o link de documentação do componente acima, se necessário também.\n",
    "    model_train_evaluate_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        # Parâmetros de autenticação Vertex AI Python SDK.      \n",
    "        project=project,\n",
    "        location=location,\n",
    "        staging_bucket=staging_bucket,\n",
    "        # WorkerPool arguments.\n",
    "        replica_count=1,\n",
    "        machine_type=\"c2-standard-4\",\n",
    "        # TODO: preencha os argumentos restantes do construtor de pipeline.\n",
    "\n",
    "    )    \n",
    "    \n",
    "    # Crie um recurso Vertex Endpoint em paralelo com o treinamento do modelo.\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "        # Parâmetros de autenticação Vertex AI Python SDK.\n",
    "        project=project,\n",
    "        location=location,\n",
    "        display_name=display_name\n",
    "    \n",
    "    )   \n",
    "    \n",
    "    # Implante seu modelo no recurso Endpoint criado para previsões online.\n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "        # Link para o componente de treinamento do modelo por meio do artefato do modelo de saída.\n",
    "        model=model_train_evaluate_op.outputs[\"model\"],\n",
    "        # Link para o Endpoint criado.\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        # Defina o roteamento de solicitação de previsão. {\"0\": 100} indica 100% do tráfego\n",
    "        # para o ID do modelo atual que está sendo implantado.\n",
    "        traffic_split={\"0\": 100},\n",
    "        # Argumentos do WorkerPool.\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783114fd-731b-4bad-bbe2-7a858e621fca",
   "metadata": {},
   "source": [
    "## Compilar o pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28dac2-3721-4fe6-9e01-98745b0d1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77355b83-577b-4831-9862-91e08e974256",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"bert-sentiment-classification.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cda30-4046-4d29-abdd-501c243f5eee",
   "metadata": {},
   "source": [
    "## Execute o pipeline no Vertex Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be420d-9d1d-4e8e-a08a-658fdfd60eb0",
   "metadata": {},
   "source": [
    "O `PipelineJob` é configurado abaixo e acionado através do método `run()`.\n",
    "\n",
    "**Observação:** essa execução de pipeline levará cerca de 30 a 40 minutos para treinar e implantar seu modelo. Acompanhe a execução usando o URL da saída do trabalho abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276575d-c2ba-4d08-9a2a-b7583af27aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
    "    display_name=\"bert-sentiment-classification\",\n",
    "    template_path=\"bert-sentiment-classification.json\",\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"location\": REGION,\n",
    "        \"staging_bucket\": GCS_BUCKET,\n",
    "        \"display_name\": DISPLAY_NAME,        \n",
    "        \"container_uri\": IMAGE_URI,\n",
    "        \"model_serving_container_image_uri\": SERVING_IMAGE_URI,        \n",
    "        \"base_output_dir\": GCS_BASE_OUTPUT_DIR},\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab35e9-207c-49ea-8a27-6e6cddce8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_pipelines_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a821a-a3bd-45bf-a9ea-aa18687218f6",
   "metadata": {},
   "source": [
    "## Modelo implantado de consulta no Vertex Endpoint para previsões online"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd8366-d362-4537-ab30-21c21fce6846",
   "metadata": {},
   "source": [
    "Por fim, você recuperará o `endpoint` implantado pelo pipeline e o usará para consultar seu modelo para previsões online.\n",
    "\n",
    "Configure a função `Endpoint()` abaixo com os seguintes parâmetros:\n",
    "\n",
    "* `endpoint_name`: um nome de recurso de terminal ou ID de terminal totalmente qualificado. Exemplo: \"projects/123/locations/us-central1/endpoints/456\" ou \"456\" quando o projeto e o local são inicializados ou passados.\n",
    "* `project_id`: projeto do GCP.\n",
    "* `local`: região do GCP.\n",
    "\n",
    "Chame `predict()` para retornar uma previsão para uma revisão de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80748b-8907-4ad6-8adb-d4c394752257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupere o nome do Endpoint implantado de seu pipeline.\n",
    "ENDPOINT_NAME = vertexai.Endpoint.list()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c989d-1026-4f57-8dac-dafad01145a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Gere previsões online usando seu Vertex Endpoint.\n",
    "\n",
    "endpoint = vertexai.Endpoint(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97759f45-e060-44ce-87fc-4d34c4b8cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: escreva uma resenha de filme para testar seu modelo, por exemplo \"The Dark Knight is the best Batman movie!\"\n",
    "test_review = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c008ce-90ad-4709-a24f-36b414c779e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use seu endpoint para retornar a previsão para seu test_review.\n",
    "prediction ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54751a3e-7b2a-4ab8-b642-6533df27de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4c68e-a937-44f8-b64a-cbe9e607cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use uma função sigmoid para compactar a saída do seu modelo entre 0 e 1. Para classificação binária, normalmente é aplicado um limite de 0,5\n",
    "# portanto, se a saída for >= 0,5, o sentimento previsto será \"Positivo\" e < 0,5 será uma previsão \"Negativa\".\n",
    "print(tf.sigmoid(prediction.predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344e3eb-0a0e-4271-b815-e792d8c95b66",
   "metadata": {},
   "source": [
    "## Próximos Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80557132-f0cf-4f4d-be5d-2c58453bc6b6",
   "metadata": {},
   "source": [
    "Parabéns! Você percorreu um fluxo de trabalho completo de experimentação, conteinerização e MLOps no Vertex AI. Primeiro, você construiu, treinou e avaliou um modelo de classificador de sentimento BERT em um Vertex Notebook. Em seguida, você empacotou o código do modelo em um contêiner do Docker para treinar no Vertex AI do Google Cloud. Por fim, você definiu e executou um Kubeflow Pipeline em Vertex Pipelines que treinou e implantou seu contêiner de modelo em um Vertex Endpoint que você consultou para previsões online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6570ed8-a1ae-41e0-8a0b-9b63ca972d85",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2c9ee-e982-4b8b-91c3-02f313896c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
